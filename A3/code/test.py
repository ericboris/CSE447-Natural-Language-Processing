# -*- coding: utf-8 -*-
"""V07_sentiment_classification_example.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1I10M7c0HNMVy4mB1V7JGx-1TZhlrm6UG

# Sentiment classification example using GRU on the imdb dataset

### CSE 447 / 517 NLP

**You can also download .py version of this notebook from [GitHub](https://github.com/ZhaofengWu/pytorch-nlp-tutorial).**

Note that in practice it is horrible style to put everything in one file.

**Before you start on Google Colab**: You probably want to use a GPU to run this notebook, as it can take many hours per epoch on CPUs, unless you are willing to use extreme hyperparameters. You can choose to use a GPU instance by clicking on *Runtime > Change runtime* type, and select *GPU* as the *hardware accelerator*. 

There are somethings that are usually done in real systems omitted in this example. For example:
1. data persistence (e.g., with pickle) so we don't have to download and process every time;
2. dropout;
3. learning rate scheduling (see torch.optim.lr_scheduler);
4. model saving/loading;
5. choosing the best epoch based on development set performance.

[Here](https://github.com/pytorch/examples) are some more official examples provided by pytorch.
"""

from google.colab import drive
drive.mount('/content/drive')

from collections import Counter
import os
import random
import tarfile
import tempfile
import urllib.request

import nltk
nltk.download('punkt')

import numpy as np
import time

import torch
from torch import nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from tqdm import tqdm
import sklearn
import math
import matplotlib.pyplot as plt

"""## Special tokens

It is important that they don't appear in the actual vocab, hence this weird look.
"""

PAD = "@@PAD@@"
UNK = "@@UNK@@"

"""## Hyperparameters 

This configuration has been tested on a 11GB GPU.

You can change them if you have a smaller GPU, in which case we recommend starting with `MAX_SEQ_LEN` and `UNK_THRESHOLD`.

If you are running this on Google Colab with a GPU instance, this configuration should be okay!
"""

MAX_SEQ_LEN = -1  # -1 for no truncation
UNK_THRESHOLD = 5
BATCH_SIZE = 128
N_EPOCHS = 8
LEARNING_RATE = 1e-3
EMBEDDING_DIM = 50
HIDDEN_DIM = 256
N_RNN_LAYERS = 2

"""## Seeding Utilities"""

def seed_everything(seed=1):
    random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = False

"""## Data Utilities"""

def download_data():
    """
    A function to download and uncompress the imdb data. You don't have to understand anything here.
    """

    def extract_data(dir, split):
        data = []
        for label in ("pos", "neg"):
            label_dir = os.path.join(dir, "aclImdb", split, label)
            files = sorted(os.listdir(label_dir))
            for file in files:
                filepath = os.path.join(label_dir, file)
                with open(filepath, encoding="UTF-8") as f:
                    data.append({"raw": f.read(), "label": label})
        return data

    url = "http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz"
    stream = urllib.request.urlopen(url)
    tar = tarfile.open(fileobj=stream, mode="r|gz")
    with tempfile.TemporaryDirectory() as td:
        tar.extractall(path=td)
        train_data = extract_data(td, "train")
        test_data = extract_data(td, "test")
        return train_data, test_data
      


def split_data(train_data, num_split=2000):
    """Splits the training data into training and development sets."""
    random.shuffle(train_data)
    return train_data[:-num_split], train_data[-num_split:]

def pretrained_embedding_loader(filename):
  with open(filename, "r") as f:
    file_reader = f.readlines()
    tokens = np.zeros((len(file_reader), 50))
    token_to_id_mapping= dict ()
    id_to_token_mapping= dict()


    for id, token_embedding in enumerate(file_reader):
        token, *embedding = token_embedding.strip().split(" ")
        tokens[id] = embedding
        id_to_token_mapping[id] = token
        token_to_id_mapping[token]  = id
  
  return tokens, token_to_id_mapping, id_to_token_mapping

def part1(filename, words_to_lookup =["dog","whale","before","however","fabricate"]):
  tokens, token_to_id_mapping, id_to_token_mapping = pretrained_embedding_loader(filename)
  similarity_mapping = dict()
  for word in words_to_lookup:
    temp_id = token_to_id_mapping[word]
    temp_emobedding = tokens[temp_id]
    #page 322 from the book
    normalized_values = np.linalg.norm(tokens ,axis=1)
    word_normlized = np.linalg.norm(temp_emobedding)
    cos_vi_vj = np.matmul(tokens, temp_emobedding) / (normalized_values * word_normlized)
    cos_vi_vj[temp_id] = -math.inf
    temp_argmax = np.argmax(cos_vi_vj)
    similarity_mapping[word] = (id_to_token_mapping[temp_argmax], cos_vi_vj[temp_argmax])
    print("The most similar word to",word,"is", similarity_mapping[word])

  return tokens, token_to_id_mapping, id_to_token_mapping, similarity_mapping, normalized_values

tokens, token_to_id_mapping, id_to_token_mapping, similarity_mapping, normalized_values = part1("/content/drive/MyDrive/447/a3/glove.6B.50d.txt")

def part2(tokens, token_to_id_mapping, id_to_token_mapping, normalized_values, analogies = [("dog","puppy","cat"), ("speak","speaker","sing"),("France","French","England"),("France","wine","England")]):
  analogies_mapping = dict()
  triple_embeddings = []
  triple_ids = []
  top_three = dict()
  
  for relation in analogies:
    triple_embeddings = []
    for relation_word in relation:
      temp_id = token_to_id_mapping[relation_word.lower()]
      triple_ids.append(temp_id)
      triple_embeddings.append(tokens[temp_id])
    temp_v_sum = -triple_embeddings[0]+triple_embeddings[1]+triple_embeddings[2]
    temp_v_sum_normalized = np.linalg.norm(temp_v_sum)
    cos_vi_vj = np.matmul(tokens, temp_v_sum) / (normalized_values * temp_v_sum_normalized)
    cos_vi_vj[triple_ids] = -math.inf
    temp_list = []
    for i in cos_vi_vj.argsort()[-3:][::-1]:
      temp_emobedding2 = tokens[i]
      #page 322 from the book
      normalized_values2 = np.linalg.norm(tokens ,axis=1)
      word_normlized2 = np.linalg.norm(temp_emobedding2)
      cos_vi_vj2 = np.matmul(tokens, temp_emobedding2) / (normalized_values2 * word_normlized2)
      cos_vi_vj2[i] = -math.inf
      temp_similar2 = id_to_token_mapping[np.argmax(cos_vi_vj2)]
      temp_argmax2 = np.argmax(cos_vi_vj2)
      similarity_entry = (id_to_token_mapping[temp_argmax2], cos_vi_vj2[temp_argmax2])
      temp_list.append(similarity_entry)
    top_three[relation] = temp_list

  return analogies_mapping, triple_ids, top_three

analogies_mapping, triple_ids, top_three = part2(tokens, token_to_id_mapping, id_to_token_mapping, normalized_values)

print(top_three)

def tokenize(data, max_seq_len=MAX_SEQ_LEN):
    """
    Here we use nltk to tokenize data. There are many possibilities. We also truncate the
    sequences so that the training time and memory is more manageable. You can think of truncation
    as making a decision only looking at the first X words.
    """
    for example in data:
        example["text"] = []
        for sent in nltk.sent_tokenize(example["raw"]):
            example["text"].extend(nltk.word_tokenize(sent))
        if max_seq_len >= 0:
            example["text"] = example["text"][:max_seq_len]


def create_vocab(data, tokens, token_to_id_mapping, unk_threshold=UNK_THRESHOLD):
    """
    Creates a vocabulary with tokens that have frequency above unk_threshold and assigns each token
    a unique index, including the special tokens.
    """
    counter = Counter(token.lower() for example in train_data for token in example["text"])
    vocab = {token for token in counter if counter[token] > 5}
    print(f"Vocab size: {len(vocab) + 2}")  # add the special tokens
    print(f"Most common tokens: {counter.most_common(10)}")
    embeddings = np.random.randn(len(vocab)+2, 50)
    print(embeddings.shape)
    token_to_idx = {PAD: 0, UNK: 1}
    id_filter = []
    for token in vocab:
      token_to_idx[token] = len(token_to_idx)
      if token in token_to_id_mapping.keys():
        temp_id=token_to_id_mapping[token]
        embeddings[len(token_to_idx)-1] = tokens[temp_id]
        id_filter.append(len(token_to_idx)-1)
    print(len(token_to_idx))
    return token_to_idx, embeddings, id_filter

def apply_vocab(data, token_to_idx):
    """
    Applies the vocabulary to the data and maps the tokenized sentences to vocab indices as the
    model input.
    """
    for example in data:
        example["text"] = [token_to_idx.get(token, token_to_idx[UNK]) for token in example["text"]]


def apply_label_map(data, label_to_idx):
    """Converts string labels to indices."""
    for example in data:
        example["label"] = label_to_idx[example["label"]]

"""Sometimes it suffices to use the pytorch built-in `TensorDataset`, but here we want to control the padding on a finer-grained level, so we implement our own. Specifically, we batch together similar-lengthed examples to minimize padding and speed up training. Most heavy-lifting is done un collate_fn that pads the tensors and batches them together."""

class SentimentDataset(Dataset):
    def __init__(self, data, pad_idx):
        data = sorted(data, key=lambda example: len(example["text"]))
        self.texts = [example["text"] for example in data]
        self.labels = [example["label"] for example in data]
        self.pad_idx = pad_idx

    def __getitem__(self, index):
        return [self.texts[index], self.labels[index]]

    def __len__(self):
        return len(self.texts)

    def collate_fn(self, batch):
        def tensorize(elements, dtype):
            return [torch.tensor(element, dtype=dtype) for element in elements]

        def pad(tensors):
            """Assumes 1-d tensors."""
            max_len = max(len(tensor) for tensor in tensors)
            padded_tensors = [
                F.pad(tensor, (0, max_len - len(tensor)), value=self.pad_idx) for tensor in tensors
            ]
            return padded_tensors

        texts, labels = zip(*batch)
        return [
            torch.stack(pad(tensorize(texts, torch.long)), dim=0),
            torch.stack(tensorize(labels, torch.long), dim=0),
        ]

"""## Model"""

class SequenceClassifier(nn.Module):
    def __init__(self, vocab_size, embedding_dim, hidden_dim, n_labels, n_rnn_layers, pad_idx, embeddigns):
        super().__init__()

        self.pad_idx = pad_idx

        self.embedding = nn.Embedding(vocab_size, embedding_dim)
        # self.embedding.weight.data = torch.tensor(embeddigns, dtype=torch.double)
        # self.embedding.weight
        print("original numpy type",embeddings.dtype)
        temp_tensor = torch.tensor(embeddings).float()
        print("tensor type",temp_tensor.dtype)
        # self.embedding = nn.Embedding.from_pretrained(temp_tensor)
        self.embedding.weight.data = temp_tensor
        print("embedding type", self.embedding.weight.data.dtype )
        self.rnn = nn.GRU(
            embedding_dim, hidden_dim, num_layers=n_rnn_layers, batch_first=True, bidirectional=True
        )
        # We take the final hidden state at all GRU layers as the sequence representation.
        # 2 because bidirectional.
        layered_hidden_dim = hidden_dim * n_rnn_layers * 2
        self.output = nn.Linear(layered_hidden_dim, n_labels)

    def forward(self, text):
        # text shape: (batch_size, max_seq_len) where max_seq_len is the max length *in this batch*
        # lens shape: (batch_size,)
        non_padded_positions = text != self.pad_idx
        lens = non_padded_positions.sum(dim=1)

        # embedded shape: (batch_size, max_seq_len, embedding_dim)
        embedded = self.embedding(text)
        # You can pass the embeddings directly to the RNN, but as the input potentially has
        # different lengths, how do you know when to stop unrolling the recurrence for each example?
        # pytorch provides a util function pack_padded_sequence that converts padded sequences with
        # potentially different lengths into a special PackedSequence object that keeps track of
        # these things. When passing a PackedSequence object into the RNN, the output will be a
        # PackedSequence too (but not the hidden state as that always has a length of 1). Since we
        # do not use the per-token output, we do not unpack it. But if you need it, e.g. for
        # token-level classification such as POS tagging, you can use pad_packed_sequence to convert
        # it back to a regular tensor.
        packed_embedded = nn.utils.rnn.pack_padded_sequence(
            embedded, lens.cpu(), batch_first=True, enforce_sorted=False
        )
        # nn.GRU produces two outputs: one is the per-token output and the other is per-sequence.
        # The pers-sequence output is simiar to the last per-token output, except that it is taken
        # at all layers.
        # output (after unpacking) shape: (batch_size, max_seq_len, hidden_dim)
        # hidden shape: (n_layers * n_directions, batch_size, hidden_dim)
        packed_output, hidden = self.rnn(packed_embedded)
        # shape: (batch_size, n_layers * n_directions * hidden_dim)
        hidden = hidden.transpose(0, 1).reshape(hidden.shape[1], -1)
        # Here we directly output the raw scores without softmax normalization which would produce
        # a valid probability distribution. This is because:
        # (1) during training, pytorch provides a loss function "F.cross_entropy" that combines
        # "log_softmax + F.nll_loss" in one step. See the `train` function below.
        # (2) during evaluation, we usually only care about the class with the highest score, but
        # not the actual probablity distribution.
        # shape: (batch_size, n_labels)
        return self.output(hidden)

def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)

"""## Training and Evaluation Functions"""

def train(model, dataloader, optimizer, device, id_filter, glove_flag=False):
    for texts, labels in tqdm(dataloader):
        texts, labels = texts.to(device), labels.to(device)
        output = model(texts)
        loss = F.cross_entropy(output, labels)
        model.zero_grad()
        loss.backward()
        if glove_flag == True:
            model.embedding.weight.grad[id_filter] = 0
        optimizer.step()


def evaluate(model, dataloader, device):
    count = correct = 0.0
    pred_list = []
    labels_list = []
    with torch.no_grad():
        for texts, labels in tqdm(dataloader):
            texts, labels = texts.to(device), labels.to(device)
            output = model(texts)
            predicted = output.argmax(dim=-1)
            pred_list.append(predicted)
            labels_list.append(labels)
            count += len(predicted)
            correct += (predicted == labels).sum().item()
    accuracy = correct / count
    print(f"Accuracy: {correct / count}")
    return accuracy, pred_list, labels_list

def plot_data(first_plot, second_plot, x_label, y_label, title, labels):
    plt.plot(first_plot, color='limegreen', label=labels[0])
    plt.plot(second_plot, color='darkviolet', label=labels[1])
    plt.xlabel(x_label)
    plt.ylabel(y_label)
    plt.title(title)
    plt.legend()
    plt.savefig(title+".png", dpi = 1000)
    plt.show()

def get_f1_macro(eval_info):
  final_pred = eval_info[-1][1]
  final_pred_tensor = torch.cat(final_pred)
  final_pred_np_array = np.array(final_pred_tensor.cpu())
  final_labels = eval_info[-1][2]
  final_labels_tensor = torch.cat(final_labels)
  final_labels_np_array = np.array(final_labels_tensor.cpu())
  f1_score = sklearn.metrics.f1_score(final_labels_np_array, final_pred_np_array, average='macro')
  return f1_score

"""## Running training and evaluation

## Part 3
"""

seed_everything()
print("Downloading data")
train_data, test_data = download_data()
train_data, dev_data = split_data(train_data)
print(f"Data sample: {train_data[:3]}")
print(f"train {len(train_data)}, dev {len(dev_data)}, test {len(test_data)}")

print("Processing data")
for data in (train_data, dev_data, test_data):
    tokenize(data)

token_to_idx, embeddings, id_filter = create_vocab(train_data, tokens, token_to_id_mapping)

# Here we only use the training data to create the vocabulary because
# (1) we shouldn't look at the test set; and
# (2) we want the dev set to accurately reflect the test set performance.
# There are people who do other things.
label_to_idx = {"neg": 0, "pos": 1}
for data in (train_data, dev_data, test_data):
    apply_vocab(data, token_to_idx)
    apply_label_map(data, label_to_idx)

pad_idx = token_to_idx[PAD]
train_dataset = SentimentDataset(train_data, pad_idx)
dev_dataset = SentimentDataset(dev_data, pad_idx)
test_dataset = SentimentDataset(test_data, pad_idx)
train_dataloader = DataLoader(
    train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=train_dataset.collate_fn
)
dev_dataloader = DataLoader(
    dev_dataset, batch_size=BATCH_SIZE, collate_fn=dev_dataset.collate_fn
)
test_dataloader = DataLoader(
    test_dataset, batch_size=BATCH_SIZE, collate_fn=test_dataset.collate_fn

)

vocab_size, embedding_dim = embeddings.shape
model = SequenceClassifier(
    vocab_size, embedding_dim, HIDDEN_DIM, len(label_to_idx), N_RNN_LAYERS, pad_idx, embeddings)
print(f"Model has {count_parameters(model)} parameters.")

# Adam is just a fancier version of SGD.
optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

def run_all(model,device, optimizer, tuned=False):
  print(f"Random baseline")
  eval_info = []
  model.eval()
  eval_info.append(evaluate(model, dev_dataloader, device))
  for epoch in range(N_EPOCHS):
      print(f"Epoch {epoch + 1}")  # 0-based -> 1-based
      model.train()
      train(model, train_dataloader, optimizer, device, id_filter, tuned)
      model.eval()
      eval_info.append(evaluate(model, dev_dataloader, device))
      if eval_info[-1][0] < eval_info[-2][0]:
        eval_info.pop(-1)
        break
  print(f"Test set performance")
  model.eval()
  test_eval = evaluate(model, test_dataloader, device)
  return eval_info, test_eval

vocab_size, embedding_dim = embeddings.shape
model2 = SequenceClassifier(
    vocab_size, embedding_dim, HIDDEN_DIM, len(label_to_idx), N_RNN_LAYERS, pad_idx
, embeddings)
print(f"Model has {count_parameters(model)} parameters.")

# Adam is just a fancier version of SGD.
optimizer2 = torch.optim.Adam(model2.parameters(), lr=LEARNING_RATE)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model2.to(device)

eval_info_untuned, test_eval_untuned = run_all(model,device, optimizer)

eval_info_tuned, test_eval_tuned = run_all(model2,device, optimizer2)

first_plot = [i[0] for i in eval_info_untuned]
second_plot = [i[0] for i in eval_info_tuned]

plot_data(first_plot, second_plot, "Epochs", "Accuracy", "Training accuracy of sentiment classifier per epoch", ["Untuned Embeddings","Finetuned Embeddings"])

f1_untuned = get_f1_macro(eval_info_untuned)
f1_finetuned = get_f1_macro(eval_info_tuned)

print("F1 macro score for untuned embeddings:", f1_untuned)
print("F1 macro score for finetuned embeddings:", f1_finetuned)

