# -*- coding: utf-8 -*-
"""
This file trains a sentiment classifier on pretrained glove word embeddings and outputs the results.
"""

"""A3_q3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FYzpUXtclluhfWrd4zq_ZXyqwDPeMlkx

# Sentiment classification example using GRU on the imdb dataset

### CSE 447 / 517 NLP

**You can also download .py version of this notebook from [GitHub](https://github.com/ZhaofengWu/pytorch-nlp-tutorial).**

Note that in practice it is horrible style to put everything in one file.

**Before you start on Google Colab**: You probably want to use a GPU to run this notebook, as it can take many hours per epoch on CPUs, unless you are willing to use extreme hyperparameters. You can choose to use a GPU instance by clicking on *Runtime > Change runtime* type, and select *GPU* as the *hardware accelerator*. 

There are somethings that are usually done in real systems omitted in this example. For example:
1. data persistence (e.g., with pickle) so we don't have to download and process every time;
2. dropout;
3. learning rate scheduling (see torch.optim.lr_scheduler);
4. model saving/loading;
5. choosing the best epoch based on development set performance.

[Here](https://github.com/pytorch/examples) are some more official examples provided by pytorch.
"""

from google.colab import drive
drive.mount('/content/drive')

from collections import Counter
import os
import random
import tarfile
import tempfile
import urllib.request

import nltk
nltk.download('punkt')

import torch
from torch import nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from tqdm import tqdm

import numpy as np
import sklearn

"""## Special tokens

It is important that they don't appear in the actual vocab, hence this weird look.
"""

PAD = "@@PAD@@"
UNK = "@@UNK@@"

"""## Hyperparameters 

This configuration has been tested on a 11GB GPU.

You can change them if you have a smaller GPU, in which case we recommend starting with `MAX_SEQ_LEN` and `UNK_THRESHOLD`.

If you are running this on Google Colab with a GPU instance, this configuration should be okay!
"""

MAX_SEQ_LEN = -1  # -1 for no truncation
UNK_THRESHOLD = 5
BATCH_SIZE = 128
N_EPOCHS = 8
LEARNING_RATE = 1e-3
EMBEDDING_DIM = 50
HIDDEN_DIM = 256
N_RNN_LAYERS = 2

"""## Seeding Utilities"""

def seed_everything(seed=1):
    random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True # TODO

"""## Data Utilities"""

def download_data():
    """
    A function to download and uncompress the imdb data. You don't have to understand anything here.
    """

    def extract_data(dir, split):
        data = []
        for label in ("pos", "neg"):
            label_dir = os.path.join(dir, "aclImdb", split, label)
            files = sorted(os.listdir(label_dir))
            for file in files:
                filepath = os.path.join(label_dir, file)
                with open(filepath, encoding="UTF-8") as f:
                    data.append({"raw": f.read(), "label": label})
        return data

    url = "http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz"
    stream = urllib.request.urlopen(url)
    tar = tarfile.open(fileobj=stream, mode="r|gz")
    with tempfile.TemporaryDirectory() as td:
        tar.extractall(path=td)
        train_data = extract_data(td, "train")
        test_data = extract_data(td, "test")
        return train_data, test_data


def split_data(train_data, num_split=2000):
    """Splits the training data into training and development sets."""
    random.shuffle(train_data)
    return train_data[:-num_split], train_data[-num_split:]

def tokenize(data, max_seq_len=MAX_SEQ_LEN):
    """
    Here we use nltk to tokenize data. There are many othe possibilities. We also truncate the
    sequences so that the training time and memory is more manageable. You can think of truncation
    as making a decision only looking at the first X words.
    """
    for example in data:
        example["text"] = []
        for sent in nltk.sent_tokenize(example["raw"]):
            example["text"].extend(nltk.word_tokenize(sent))
        if max_seq_len >= 0:
            example["text"] = example["text"][:max_seq_len]


def create_vocab(data, unk_threshold=UNK_THRESHOLD):
    """
    Creates a vocabulary with tokens that have frequency above unk_threshold and assigns each token
    a unique index, including the special tokens.
    """
    counter = Counter(token.lower() for example in data for token in example["text"])
    vocab = {token for token in counter if counter[token] > unk_threshold}
    print(f"Vocab size: {len(vocab) + 2}")  # add the special tokens
    print(f"Most common tokens: {counter.most_common(10)}")
    token_to_idx = {PAD: 0, UNK: 1}
    for token in vocab:
        token_to_idx[token] = len(token_to_idx)
    return token_to_idx


def apply_vocab(data, token_to_idx):
    """
    Applies the vocabulary to the data and maps the tokenized sentences to vocab indices as the
    model input.
    """
    for example in data:
        example["text"] = [token_to_idx.get(token, token_to_idx[UNK]) for token in example["text"]]


def apply_label_map(data, label_to_idx):
    """Converts string labels to indices."""
    for example in data:
        example["label"] = label_to_idx[example["label"]]

def load_pretrained_embeddings(file_name, verbose=False):
    ''' Return the lines from the given file as a word->numpy array mapping. '''
    if verbose:
        print('Loading word embeddings')
    embeds = {}
    with open(file_name) as f:
        for line in f:
            line = line.split()
            key = line[0]
            # Convert the list of strings to a numpy array of floats.
            val = np.array(list(map(float, line[1:])))
            embeds[key] = val
    return embeds

def extend_embeddings(embeds, new_tokens, verbose=False):
    ''' Return the given embeddings extended with random word embedding vectors 
        for the given new tokens. '''
    if verbose:
        print('Extending embeddings')
    for token in new_tokens:
        if token not in embeds:
            # Let the token's random vector embedding be a rows=1 by cols=50 np array.
            embeds[token] = np.random.randn(1, 50)[0]
    return embeds
        
def unseen_types(embeds, train_data, verbose=False):
    ''' Return a set of words that are in train_data but not in embeds. '''
    return set([t for t in train_data if t not in embeds])

def embedding_matrix(embeds, verbose=False):
    ''' Given a dictionary of word embeddings return a 2d numpy array of the word embedding vectors. '''
    if verbose:
        print('Embedding matrix')
    array = []
    for key in embeds:
        array.append(embeds[key])
    return np.array(array)

def embedding_filter(embeds, new_tokens, verbose=False):
    ''' Return a list of indices of embeds excluding any that appear in new_tokens. '''
    if verbose:
        print('Embedding filter')
    filter = []
    for i, key in enumerate(embeds):
        if key not in new_tokens:
            filter.append(i)
    return filter

"""Sometimes it suffices to use the pytorch built-in `TensorDataset`, but here we want to control the padding on a finer-grained level, so we implement our own. Specifically, we batch together similar-lengthed examples to minimize padding and speed up training. Most heavy-lifting is done un collate_fn that pads the tensors and batches them together."""

class SentimentDataset(Dataset):
    def __init__(self, data, pad_idx):
        data = sorted(data, key=lambda example: len(example["text"]))
        self.texts = [example["text"] for example in data]
        self.labels = [example["label"] for example in data]
        self.pad_idx = pad_idx

    def __getitem__(self, index):
        return [self.texts[index], self.labels[index]]

    def __len__(self):
        return len(self.texts)

    def collate_fn(self, batch):
        def tensorize(elements, dtype):
            return [torch.tensor(element, dtype=dtype) for element in elements]

        def pad(tensors):
            """Assumes 1-d tensors."""
            max_len = max(len(tensor) for tensor in tensors)
            padded_tensors = [
                F.pad(tensor, (0, max_len - len(tensor)), value=self.pad_idx) for tensor in tensors
            ]
            return padded_tensors

        texts, labels = zip(*batch)
        return [
            torch.stack(pad(tensorize(texts, torch.long)), dim=0),
            torch.stack(tensorize(labels, torch.long), dim=0),
        ]

"""## Model"""

class SequenceClassifier(nn.Module):
    def __init__(self, vocab_size, embedding_dim, hidden_dim, n_labels, n_rnn_layers, pad_idx, embeddings):
        super().__init__()

        self.pad_idx = pad_idx

        self.embedding = nn.Embedding(vocab_size, embedding_dim)

        self.embedding.weight.data = torch.tensor(embeddings).float()

        self.rnn = nn.GRU(
            embedding_dim, hidden_dim, num_layers=n_rnn_layers, batch_first=True, bidirectional=True
        )
        # We take the final hidden state at all GRU layers as the sequence representation.
        # 2 because bidirectional.
        layered_hidden_dim = hidden_dim * n_rnn_layers * 2
        self.output = nn.Linear(layered_hidden_dim, n_labels)

    def forward(self, text):
        # text shape: (batch_size, max_seq_len) where max_seq_len is the max length *in this batch*
        # lens shape: (batch_size,)
        non_padded_positions = text != self.pad_idx
        lens = non_padded_positions.sum(dim=1)

        # embedded shape: (batch_size, max_seq_len, embedding_dim)
        embedded = self.embedding(text)
        # You can pass the embeddings directly to the RNN, but as the input potentially has
        # different lengths, how do you know when to stop unrolling the recurrence for each example?
        # pytorch provides a util function pack_padded_sequence that converts padded sequences with
        # potentially different lengths into a special PackedSequence object that keeps track of
        # these things. When passing a PackedSequence object into the RNN, the output will be a
        # PackedSequence too (but not the hidden state as that always has a length of 1). Since we
        # do not use the per-token output, we do not unpack it. But if you need it, e.g. for
        # token-level classification such as POS tagging, you can use pad_packed_sequence to convert
        # it back to a regular tensor.
        packed_embedded = nn.utils.rnn.pack_padded_sequence(
            embedded, lens.cpu(), batch_first=True, enforce_sorted=False
        )
        # nn.GRU produces two outputs: one is the per-token output and the other is per-sequence.
        # The pers-sequence output is simiar to the last per-token output, except that it is taken
        # at all layers.
        # output (after unpacking) shape: (batch_size, max_seq_len, hidden_dim)
        # hidden shape: (n_layers * n_directions, batch_size, hidden_dim)
        packed_output, hidden = self.rnn(packed_embedded)
        # shape: (batch_size, n_layers * n_directions * hidden_dim)
        hidden = hidden.transpose(0, 1).reshape(hidden.shape[1], -1)
        # Here we directly output the raw scores without softmax normalization which would produce
        # a valid probability distribution. This is because:
        # (1) during training, pytorch provides a loss function "F.cross_entropy" that combines
        # "log_softmax + F.nll_loss" in one step. See the `train` function below.
        # (2) during evaluation, we usually only care about the class with the highest score, but
        # not the actual probablity distribution.
        # shape: (batch_size, n_labels)
        return self.output(hidden)

def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)

"""## Training and Evaluation Functions"""

def train(model, dataloader, optimizer, device, filter=None):
    for texts, labels in tqdm(dataloader):
        texts, labels = texts.to(device), labels.to(device)
        output = model(texts)
        loss = F.cross_entropy(output, labels)
        model.zero_grad()
        loss.backward()
        if filter:
            model.embedding.weight.grad[filter] = 0
        optimizer.step()


def evaluate(model, dataloader, device):
    count = correct = 0.0
    predicted_labels = []
    actual_labels = []
    with torch.no_grad():
        for texts, labels in tqdm(dataloader):
            texts, labels = texts.to(device), labels.to(device)
            # shape: (batch_size, n_labels)
            output = model(texts)
            # shape: (batch_size,)
            predicted = output.argmax(dim=-1)
            count += len(predicted)
            correct += (predicted == labels).sum().item()
            predicted_labels.append(predicted)
            actual_labels.append(labels)
    accuracy = correct / count

    # Convert the labels for sklearn.
    actual_labels = np.array(torch.cat(actual_labels).cpu())
    predicted_labels = np.array(torch.cat(predicted_labels).cpu())
    f1 = sklearn.metrics.f1_score(actual_labels, predicted_labels, average='macro')

    return accuracy, f1

"""## Running training and evaluation"""

def main():
    # Load the pretrained glove word embeddings as a token -> numpy array mapping.
    pretrained_embeds_path = "/content/drive/MyDrive/glove.6B.50d.txt"
    pretrained_embeds = load_pretrained_embeddings(pretrained_embeds_path, verbose=True)
    
    seed_everything()
    
    print("Downloading data")
    train_data, test_data = download_data()
    train_data, dev_data = split_data(train_data)
    print(f"Data sample: {train_data[:3]}")
    print(f"train {len(train_data)}, dev {len(dev_data)}, test {len(test_data)}")
    
    print("Processing data")
    for data in (train_data, dev_data, test_data):
        tokenize(data)
    
    # Here we only use the training data to create the vocabulary because
    # (1) we shouldn't look at the test set; and
    # (2) we want the dev set to accurately reflect the test set performance.
    # There are people who do other things.
    token_to_idx = create_vocab(train_data)
    
    # Build a set of words that are in the training set but aren't in the 
    # pretrained word embeddings.
    new_types = unseen_types(pretrained_embeds, token_to_idx, verbose=True)
    
    # Extend the word embeddings to include randomized word embedding vectors
    # for the new types.
    extended_embeddings = extend_embeddings(pretrained_embeds, new_types, verbose=True)

    # Let the following be a filter for ignoring certain tokens while training. 
    filter = embedding_filter(extended_embeddings, new_types, verbose=True)

    # Convert the dictionary of word vector embeddings to a single 2d numpy 
    # array of vectors.
    embeddings = embedding_matrix(extended_embeddings, verbose=True)
    
    label_to_idx = {"neg": 0, "pos": 1}
    for data in (train_data, dev_data, test_data):
        apply_vocab(data, token_to_idx)
        apply_label_map(data, label_to_idx)
    
    pad_idx = token_to_idx[PAD]
    train_dataset = SentimentDataset(train_data, pad_idx)
    dev_dataset = SentimentDataset(dev_data, pad_idx)
    test_dataset = SentimentDataset(test_data, pad_idx)
    train_dataloader = DataLoader(
        train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=train_dataset.collate_fn
    )
    dev_dataloader = DataLoader(
        dev_dataset, batch_size=BATCH_SIZE, collate_fn=dev_dataset.collate_fn
    )
    test_dataloader = DataLoader(
        test_dataset, batch_size=BATCH_SIZE, collate_fn=test_dataset.collate_fn
    )

    def train_and_test(model, optimizer, device, filter=None):
        # Store the each training iteration's accuracy.
        prev_train_accuracy = float('-inf')
        prev_train_f1 = float('-inf')

        print(f"Random baseline")
        evaluate(model, dev_dataloader, device)
        for epoch in range(N_EPOCHS):
            print(f"Epoch {epoch + 1}")  # 0-based -> 1-based
            train(model, train_dataloader, optimizer, device, filter)
            accuracy, f1 = evaluate(model, dev_dataloader, device)

            # Stop training when accuracy decreases.
            if accuracy < prev_train_accuracy:
                break
            prev_train_accuracy = accuracy
            prev_train_f1 = f1
        print(f'Train Accuracy: {prev_train_accuracy}\tTrain f1: {prev_train_f1}')

        print(f"Test set performance")
        test_accuracy, test_f1 = evaluate(model, test_dataloader, device)
        print(f'Test Accuracy: {test_accuracy}\tTest f1: {test_f1}')
    
    # Build a model
    model = SequenceClassifier(
        len(token_to_idx), EMBEDDING_DIM, HIDDEN_DIM, len(label_to_idx), N_RNN_LAYERS, pad_idx, embeddings
    )
    print(f"Model has {count_parameters(model)} parameters.")

    # Adam is just a fancier version of SGD.
    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)

    # Train an untuned model.
    train_and_test(model, optimizer, device, filter)

    # Build a model
    model = SequenceClassifier(
        len(token_to_idx), EMBEDDING_DIM, HIDDEN_DIM, len(label_to_idx), N_RNN_LAYERS, pad_idx, embeddings
    )
    print(f"Model has {count_parameters(model)} parameters.")
    
    # Adam is just a fancier version of SGD.
    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)

    # Train a tuned model.
    # Only difference from the untuned model is the lack of a filter.
    train_and_test(model, optimizer, device)
 
    
if __name__ == '__main__':
    main()
