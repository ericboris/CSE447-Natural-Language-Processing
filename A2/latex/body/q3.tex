\textbf{3}. Consider the same network as in problem 2 (with ReLU activations for the hidden layer), with an arbitrary differentiable loss function $\ell(y^{(i)},\tilde{y})$, where $\tilde{y}$ is the activation of the output node. Suppose all weights and offsets are initialized to zero. Show that gradient descent will not learn the desired function from this initialization.
