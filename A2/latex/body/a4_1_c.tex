\begin{quote}
    The results show that {\bf increasing n} is correlated with {\bf decreased perplexity} across all datasets but with {\bf increased percentage of words removed} on validation and testing datasets. This is to be expected. Increasing n increases the size of the model which increases the number of permutations of words the model can recognize and thus decrease the model's perplexity to novel phrases. Increasing n increases the size of the model but also the pecularity of a phrase. I.e. suppose a unigram model was trained on the phrase "this and that" and was shown the test phrase "that and this". No tokens will be removed. However if a trigram model is trained on the same phrase and the same test phrase is shown to this model, all the tokens will be removed because the percentage associated with this latter phrase in the trigram model is 0 because this phrase hadn't been seen.
\end{quote}
