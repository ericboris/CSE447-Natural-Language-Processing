\begin{quote}
    Perplexity and Percentage of Tokens Removed by Linear Interpolation Trigram models with different Lambda Parameters on Training, Validation, and Testing datasets with an unk threshold of 3.
\end{quote}
\begin{center}
    \begin{tabular}{ |c|c|c|c| } 
	\hline
	$\lambda_1$, $\lambda_2$, $\lambda_3$ & Training & Validation & Testing \\ 
	\hline
	0.0, 0.1, 0.9 & 7.82 & 43.79 & 43.58 \\ 
	\hline
	0.1, 0.3, 0.6 & 10.40 & 289.71 & 288.47 \\ 
	\hline
	0.3, 0.4, 0.3 & 16.94 & 230.65 & 230.19 \\ 
	\hline
	0.6, 0.3, 0.1 & 36.22 & 257.18 & 257.30 \\ 
	\hline
	0.9, 0.1, 0.0 & 282.09 & 437.69 & 440.06 \\ 
	\hline
    \end{tabular}
\end{center}
\begin{quote}
    The trend is that {\bf increasing $\lambda_3$ decreases perplexity} while {\bf increasing $\lambda_1$ increases perplexity}.
\end{quote}
